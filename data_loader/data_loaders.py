import os
import sys
import torch
import pefile
import multiprocessing
import numpy as np
import pandas as pd
import pickle as pkl
from tqdm import tqdm
from pathlib import Path
from base import BaseDataLoader
from torch.utils.data import Dataset

from data_loader.vectorizer import BM25
from pinecone_text.sparse import BM25Encoder
from sklearn.feature_extraction.text import TfidfVectorizer


from typing import Optional, Union, List, Tuple, TypedDict
from capstone import Cs, CS_ARCH_X86, CS_MODE_32, CS_MODE_64


class MalwareDetectionDataset(Dataset):
    def __init__(self, 
            metadata_path: Optional[Union[str, os.PathLike]], 
            data_dir: Optional[Union[str, os.PathLike]],
            num_workers:int,
            logger,
            save_dir,
            n_gram
        ):
        self.logger = logger
        self.num_workers = num_workers
        self.n_gram = n_gram
        self._metadata_path = Path(metadata_path)
        self._data_dir = Path(data_dir)
        assert self._metadata_path.exists(), "cannot find metadata.csv, this file must be exists."
        assert self._data_dir.exists(), "cannot find dataset directory, this file must be exists."
        
        self._save_dir = Path(save_dir)
        self._save_dir.mkdir(parents=True, exist_ok=True)
        
        self._metadata = pd.read_csv(self._metadata_path)
        
        self._passage = None
        self._get_opcode_sequences(self._data_dir, self._metadata, self.num_workers)
        
        self._embed = None
        self._embed_passage = None
        self._passage_embedding(self.passage)
    
    def _get_opcode_sequences(self, data_dir:Path, metadata, num_workers:int) -> List[Tuple]:
        seq_path = str(self._save_dir / 'not_embed.bin')
        if os.path.exists(seq_path):
            with open(seq_path, 'rb') as f:
                message = "passage already exists. load passage from {}".format(seq_path)
                self.logger.info(message)
                self._passage = pkl.load(f)
        else:
            self._passage = []
            with multiprocessing.Pool(processes=num_workers) as pool:
                for _, row in tqdm(metadata.iterrows(), desc='pefile to opcode...', total=len(metadata)):
                    fname, label = row['file_name'], row['label']
                    fpath = data_dir / fname
                    if not os.path.exists(fpath):
                        message = "Warning: {} not in {}, ignored.".format(fname, str(data_dir))
                        self.logger.warning(message)
                    else:
                        result = pool.apply_async(self._process_file, args=(fpath, label))
                        self._passage.append(result.get())    # wait for the process to complete 
                                
            with open(seq_path, 'wb') as f:
                message = "save not_embed passage, save path is {}".format(seq_path)
                self.logger.info(message)
                pkl.dump(self._passage, f)
    
    def _process_file(self, fpath, label):
        pe = pefile.PE(fpath)
        opcode_sequence = self._get_opcode_sequence(pe)
        return (fpath.name, label, opcode_sequence)

    def _get_opcode_sequence(self, pe:pefile.PE):
        ext = []
        for section in pe.sections:
            section_name = section.Name.decode('utf-8', errors='ignore').strip('\x00')
            if section_name == '.text':     # code section
                code = section.get_data()

                # set disassembler architecture
                if pe.FILE_HEADER.Machine == 0x14C:  # 32bit
                    md = Cs(CS_ARCH_X86, CS_MODE_32)
                elif pe.FILE_HEADER.Machine == 0x8664:  # 64bit
                    md = Cs(CS_ARCH_X86, CS_MODE_64)
                else:
                    print("Unsupported architecture")
                    sys.exit(-1)

                # append opcode sequence
                for op in md.disasm(code, section.VirtualAddress):
                    ext.append(op.mnemonic)

        context = ' '.join(ext)
        return context
    
    def _passage_embedding(self, passage):
        """ passage embedding using bm25 """
        
        embed_path = str(self._save_dir / 'embed.bin')
        if os.path.exists(embed_path):
            with open(embed_path, 'rb') as f:
                message = "embed already exists. load embed from {}".format(embed_path)
                self.logger.info(message)
                self._embed = pkl.load(f)
        else:
            psg = list(map(lambda x: x[-1], passage))
            # ======================================== #
            # encode with TFIDF using spacy tokenizer  #
            # ======================================== #
            
            # self._embed = BM25(ngram_range=(1, self.n_gram))
            self._embed = TfidfVectorizer(ngram_range=(self.n_gram, self.n_gram))
            self._embed.fit(psg)
            with open(embed_path, 'wb') as f:
                message = 'save embed object, save path is {}'.format(embed_path)
                self.logger.info(message)
                pkl.dump(self._embed, f)
        
        embed_passage_path = str(self._save_dir / 'embed_passage.bin')
        if os.path.exists(embed_passage_path):
            with open(embed_passage_path, 'rb') as f:
                message = "embed passage already exists. load embed from {}".format(embed_passage_path)
                self.logger.info(message)
                self._embed_passage = pkl.load(f)
        else:
            file_names = list(map(lambda x: x[0], passage))
            embed_passage = self._embed.transform(list(map(lambda x: x[-1], passage)))
            labels = list(map(lambda x: x[1], passage))
            self._embed_passage = {
                "file_name": file_names,
                "embed_passage": embed_passage,
                "label": labels
            }
            
            with open(embed_passage_path, 'wb') as f:
                message = 'save embed passage object, save path is {}'.format(embed_path)
                self.logger.info(message)
                pkl.dump(self._embed_passage, f)

    @property
    def passage(self):
        return self._passage
    
    @property
    def embed(self):
        return self._embed
    
    @property
    def embed_passage(self):
        return self._embed_passage
    
    @property
    def metadata(self):
        return self._metadata

    def __len__(self):
        return len(self.passage)
    
    def __getitem__(self, idx):
        # using for dataloader
        embed_passage = self.embed_passage['embed_passage']
        return {
            'file_name': self.embed_passage['file_name'][idx],
            'embed_passage': torch.tensor(embed_passage[idx].toarray()).to(dtype=torch.float).squeeze(),
            'label': torch.tensor(self.embed_passage['label'][idx])
        }
    
    
class MalwareDetectionDataLoader(BaseDataLoader):
    """ MalwareDetection data loader """
    def __init__(self, 
            data_dir, 
            metadata_path, 
            logger, 
            batch_size=1, 
            validation_split:Union[float, int]=0.0,
            num_workers:int=1,
            n_gram:int=1,
            save_dir: Union[str, os.PathLike]='saved/passage',  # object save dir
            shuffle=True, 
            training=True
        ):
        self.dataset = MalwareDetectionDataset(
            data_dir=data_dir,
            metadata_path=metadata_path,
            num_workers=num_workers,
            n_gram=n_gram,
            logger=logger,
            save_dir=save_dir
        )
        super().__init__(self.dataset, batch_size, shuffle, validation_split, num_workers)
    

    